{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_gap.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgZVZWDucXnCsSrT1LoFB7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingsgameBen/MLbasic_0223/blob/main/sentiment_gap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6FPaZmGJeOa"
      },
      "source": [
        "# 詞嵌入\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYrjHKFnJKdu"
      },
      "source": [
        "import tensorflow as tf\n",
        "dataset = tf.keras.utils.get_file(\n",
        "    fname=\"aclImdb.tar.gz\", \n",
        "    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "    extract=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcE5ojY3Jc5w"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "print(glob.glob(dataset))\n",
        "print(os.path.dirname(dataset))\n",
        "dn = os.path.dirname(dataset)\n",
        "fn = glob.glob(dn+\"/aclImdb/train/pos/*\")\n",
        "with open(fn[0], \"r\", encoding=\"utf-8\") as f:\n",
        "    print(\"pos:\", f.read())\n",
        "\n",
        "fn = glob.glob(dn+\"/aclImdb/train/neg/*\")\n",
        "with open(fn[0], \"r\", encoding=\"utf-8\") as f:\n",
        "    print(\"neg:\", f.read())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjUgesrfONOa"
      },
      "source": [
        "import pandas as pd\n",
        "base = os.path.join(dn, \"aclImdb\", \"train\")\n",
        "def getdata(base):\n",
        "    datas = {\"article\":[], \"ans\":[]}\n",
        "    targets = os.path.join(base, \"pos\", \"*\")\n",
        "    for fn in  glob.glob(targets):\n",
        "        with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
        "            datas[\"article\"].append(f.read())\n",
        "            datas[\"ans\"].append(1)\n",
        "    targets = os.path.join(base, \"neg\", \"*\")\n",
        "    for fn in glob.glob(targets):\n",
        "        with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
        "            datas[\"article\"].append(f.read())\n",
        "            datas[\"ans\"].append(0)\n",
        "    return pd.DataFrame(datas)\n",
        "train = os.path.join(dn, \"aclImdb\", \"train\")\n",
        "train_df = getdata(train)\n",
        "test = os.path.join(dn, \"aclImdb\", \"test\")\n",
        "test_df = getdata(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkM79fn2O4pV"
      },
      "source": [
        "print(len(train_df))\n",
        "print(len(test_df))\n",
        "print(test_df)\n",
        "train_df[\"article\"][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flrpcGZxbvYP"
      },
      "source": [
        "https://keras.io/zh/preprocessing/text/\n",
        "\n",
        "介系詞、主詞等不需要排除掉,因為在訓練及反向傳遞時，會調整這些常用字的權重(因為在正、負面文章都會出現這些字)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnZg5gW2afIA"
      },
      "source": [
        "# Tokenize: I->1 ..love ->2.. you->3 ...\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tok = Tokenizer(num_words=4000)\n",
        "tok.fit_on_texts(train_df[\"article\"])  #fit\n",
        "x_train_seq = tok.texts_to_sequences(train_df[\"article\"])  #transform\n",
        "x_test_seq = tok.texts_to_sequences(test_df[\"article\"])  #transform\n",
        "tok.index_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8T3p2VifZ9x"
      },
      "source": [
        "pd.DataFrame(x_train_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkLlW04EeQ1L"
      },
      "source": [
        "# tok.word_index, 0不會用到, 0 for padding\n",
        "# \" \".join([tok.index_word[n] for n in x_train_seq[0]])\n",
        "# tok.sequences_to_texts(x_train_seq)[0]\n",
        "print(tok.index_word[19])\n",
        "# 有東西消失是因為超過三千最常出現單字\n",
        "tok.sequences_to_texts(x_train_seq)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHS_VoU0cxwR"
      },
      "source": [
        "tok.index_word[783]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1lysHOggA2k"
      },
      "source": [
        "https://keras.io/zh/preprocessing/sequence/\n",
        "\n",
        "0 就只能拿來做padding\n",
        "\n",
        "1個單字token: 1個數字 \\\n",
        "1個序列sequence: 1組token(很多個數字) \\\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAq64XMzCo2f"
      },
      "source": [
        "MAXLEN = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y27-geQRf2xp"
      },
      "source": [
        "# 截長補短 pad_sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "x_train_pad = pad_sequences(x_train_seq, maxlen=MAXLEN)\n",
        "x_test_pad = pad_sequences(x_test_seq, maxlen=MAXLEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFAP0wCLkEx8"
      },
      "source": [
        "pd.DataFrame(x_train_pad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq-oo1mfkl7h"
      },
      "source": [
        "https://keras.io/zh/layers/embeddings/\n",
        "\n",
        "input_dim: 3000+1 (0 padding) \\\n",
        "output_dim:128 or 256 or 512 個情緒  \\\n",
        "input_length: 1024  !! \\\n",
        "mask_zero: 設定成true時, 設0的padding填補值, 就不會被考慮進去 \\\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjxKqFFDkHNR"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D\n",
        "layers = [\n",
        "          Embedding(4001, 128, mask_zero=True, input_length=MAXLEN),\n",
        "          GlobalAveragePooling1D(),\n",
        "          Dense(2, activation=\"softmax\")\n",
        "]\n",
        "model = Sequential(layers)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfdRbrUEBX2J"
      },
      "source": [
        "# one-hot encoding讓它自己搞定, 不需要自己做One-Hot\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "model.compile(loss=SparseCategoricalCrossentropy(),\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROOCWQfKBtuh"
      },
      "source": [
        "import numpy as np\n",
        "y_train = np.array(train_df[\"ans\"])\n",
        "y_test = np.array(test_df[\"ans\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQBIkhIABl4o"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "callbacks = [\n",
        "             EarlyStopping(patience=5, restore_best_weights=True),\n",
        "             ModelCheckpoint(\"sentiment.h5\", save_best_only=True)\n",
        "]\n",
        "model.fit(x_train_pad,\n",
        "          y_train,\n",
        "          batch_size=200,\n",
        "          epochs=50,\n",
        "          validation_split=0.1,\n",
        "          verbose=2,\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnDJ-_z9CTGx"
      },
      "source": [
        "model.evaluate(x_test_pad, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spd0eCZ4Dh2x"
      },
      "source": [
        "# 延伸補充"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H4oAWj8Cosc"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D\n",
        "layers = [\n",
        "          Embedding(4001, 128, mask_zero=True),\n",
        "          GlobalAveragePooling1D()\n",
        "]\n",
        "\n",
        "temp = Sequential(layers)\n",
        "temp.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyuwRfY2ET8q"
      },
      "source": [
        "w = model.layers[0].get_weights()\n",
        "print(w[0].shape)\n",
        "temp.layers[0].set_weights(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYpuVkiCEv7S"
      },
      "source": [
        "pre = temp.predict([[1]])\n",
        "print(pre[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnao6sAKG8BT"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "w1 = tok.word_index[\"love\"]\n",
        "pre = temp.predict([[w1]])\n",
        "e1 = pre[0]\n",
        "print(w1, e1)\n",
        "w2 = tok.word_index[\"liv\n",
        "\\\"]\n",
        "pre = temp.predict([[w2]])\n",
        "e2 = pre[0]\n",
        "# print(w2, e2)\n",
        "print(\"-\"*80)\n",
        "print(w1, w2)\n",
        "cos_sim = dot(e1, e2)/(norm(e1)*norm(e2))\n",
        "print(\"相似度\", cos_sim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fnbtoJoKM8k"
      },
      "source": [
        "# s1 = \"this movie is interesting\"\n",
        "s1 = \"i love this movie\"\n",
        "w1 = [tok.word_index[w] for w in s1.split(\" \")]\n",
        "pre = temp.predict([w1])\n",
        "e1 = pre[0]\n",
        "# print(w1, e1)\n",
        "# s2 = \"this movie is boring\"\n",
        "s2 = \"i love this movie badly\"\n",
        "w2 = [tok.word_index[w] for w in s2.split(\" \")]\n",
        "pre = temp.predict([w2])\n",
        "e2 = pre[0]\n",
        "# print(w2, e2)\n",
        "print(\"-\"*80)\n",
        "print(w1, w2)\n",
        "cos_sim = dot(e1, e2)/(norm(e1)*norm(e2))\n",
        "print(\"相似度\", cos_sim)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}