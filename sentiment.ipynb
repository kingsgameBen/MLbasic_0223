{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2G27lkuTntHfM+h9IC8B/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingsgameBen/MLbasic_0223/blob/main/sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6FPaZmGJeOa"
      },
      "source": [
        "# 詞嵌入\n",
        "\n",
        "unsup = unsupervised 非監督式\n",
        "\n",
        "答案是自己產生的 ＝ 半監督式\n",
        "\n",
        "深度學習必給答案, 除非你是遷移學習(偷別人的模型)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYrjHKFnJKdu"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\n",
        "    fname=\"aclImdb.tar.gz\", \n",
        "    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "    extract=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcE5ojY3Jc5w"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import textwrap\n",
        "print(glob.glob(dataset))\n",
        "print(os.path.dirname(dataset))\n",
        "dn = os.path.dirname(dataset)\n",
        "print(dn)\n",
        "# print(glob.glob(dn+\"/*\"))\n",
        "# print(glob.glob(dn+\"/aclImdb/*\"))\n",
        "# print(glob.glob(dn+\"/aclImdb/train/*\"))\n",
        "fn = glob.glob(dn+\"/aclImdb/train/pos/*\")\n",
        "print(fn)\n",
        "with open(fn[0], \"r\", encoding=\"utf-8\") as f:\n",
        "    print(\"pos:\", textwrap.fill(f.read(), width=80))\n",
        "print(\"=\"*80)\n",
        "fn = glob.glob(dn+\"/aclImdb/train/neg/*\")\n",
        "with open(fn[0], \"r\", encoding=\"utf-8\") as f:\n",
        "    print(\"neg:\", textwrap.fill(f.read(), width=80))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjUgesrfONOa"
      },
      "source": [
        "import pandas as pd\n",
        "base = os.path.join(dn, \"aclImdb\", \"train\")\n",
        "print(base)\n",
        "def getdata(base):\n",
        "    datas = {\"article\":[], \"ans\":[]}\n",
        "    targets = os.path.join(base, \"pos\", \"*\")\n",
        "    print(targets)\n",
        "    for fn in  glob.glob(targets):\n",
        "        with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
        "            datas[\"article\"].append(f.read())\n",
        "            datas[\"ans\"].append(1)\n",
        "    targets = os.path.join(base, \"neg\", \"*\")\n",
        "    for fn in glob.glob(targets):\n",
        "        with open(fn, \"r\", encoding=\"utf-8\") as f:\n",
        "            datas[\"article\"].append(f.read())\n",
        "            datas[\"ans\"].append(0)\n",
        "    return pd.DataFrame(datas)\n",
        "train = os.path.join(dn, \"aclImdb\", \"train\")\n",
        "train_df = getdata(train)\n",
        "test = os.path.join(dn, \"aclImdb\", \"test\")\n",
        "test_df = getdata(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkM79fn2O4pV"
      },
      "source": [
        "print(len(train_df))\n",
        "print(len(test_df))\n",
        "print(test_df)\n",
        "print(textwrap.fill(train_df[\"article\"][20000]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flrpcGZxbvYP"
      },
      "source": [
        "https://keras.io/zh/preprocessing/text/\n",
        "\n",
        "介系詞、主詞等不需要排除掉,因為在訓練及反向傳遞時，會調整這些常用字的權重(因為在正、負面文章都會出現這些字)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnZg5gW2afIA"
      },
      "source": [
        "# Tokenize: I->1 ..love ->2.. you->3 ...\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tok = Tokenizer(num_words=3000)\n",
        "tok.fit_on_texts(train_df[\"article\"])  #fit\n",
        "x_train_seq = tok.texts_to_sequences(train_df[\"article\"])  #transform\n",
        "x_test_seq = tok.texts_to_sequences(test_df[\"article\"])  #transform\n",
        "tok.index_word\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8T3p2VifZ9x"
      },
      "source": [
        "pd.DataFrame(x_train_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkLlW04EeQ1L"
      },
      "source": [
        "# 有些字消失，是因為超過最常出現的3000個單字\n",
        "# tok.word_index, 0不會用到, 0 for padding\n",
        "compare_vanish_words = \" \".join([tok.index_word[n] for n in x_train_seq[0]])\n",
        "print(textwrap.fill(compare_vanish_words, width=100))\n",
        "# print(tok.sequences_to_texts(x_train_seq)[0]) #跟上面兩行效果相同, 但程式碼更簡單\n",
        "\n",
        "tok.index_word[19]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHS_VoU0cxwR"
      },
      "source": [
        "# print(tok.word_index) # 0不會用到,只能拿來padding\n",
        "tok.index_word[783]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1lysHOggA2k"
      },
      "source": [
        "https://keras.io/zh/preprocessing/sequence/\n",
        "\n",
        "0 就只能拿來做padding\n",
        "\n",
        "1個單字token: 1個數字 \\\n",
        "1個序列sequence: 1組token(很多個數字) \\\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAq64XMzCo2f"
      },
      "source": [
        "MAXLEN = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y27-geQRf2xp"
      },
      "source": [
        "# 截長補短 pad_sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "x_train_pad = pad_sequences(x_train_seq, maxlen=MAXLEN)\n",
        "x_test_pad = pad_sequences(x_test_seq, maxlen=MAXLEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFAP0wCLkEx8"
      },
      "source": [
        "pd.DataFrame(x_train_pad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq-oo1mfkl7h"
      },
      "source": [
        "https://keras.io/zh/layers/embeddings/\n",
        "\n",
        "input_dim: 3000+1 (0 padding) \\\n",
        "output_dim:128 or 256 or 512 個情緒  \\\n",
        "input_length: 1024  !! \\\n",
        "mask_zero: 設定成true時(遮住數字0), 設0的padding填補值, 就不會被考慮進去 \\\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjxKqFFDkHNR"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "# 線性分類器模型(沒有ReLU、Sigmoid)\n",
        "layers = [\n",
        "          Embedding(3001, 128, mask_zero=True, input_length=MAXLEN),\n",
        "          Flatten(),\n",
        "          Dense(256, activation=\"relu\"),\n",
        "          Dense(2, activation=\"softmax\")\n",
        "]\n",
        "model = Sequential(layers)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfdRbrUEBX2J"
      },
      "source": [
        "# one-hot encoding讓它自己搞定, 不需要自己做One-Hot\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "model.compile(loss=SparseCategoricalCrossentropy(),\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROOCWQfKBtuh"
      },
      "source": [
        "import numpy as np\n",
        "y_train = np.array(train_df[\"ans\"])\n",
        "y_test = np.array(test_df[\"ans\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQBIkhIABl4o"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "callbacks = [\n",
        "             EarlyStopping(patience=5, restore_best_weights=True),\n",
        "             ModelCheckpoint(\"sentiment.h5\", save_best_only=True)\n",
        "]\n",
        "model.fit(x_train_pad,\n",
        "          y_train,\n",
        "          batch_size=200,\n",
        "          epochs=50,\n",
        "          validation_split=0.1,\n",
        "          verbose=2,\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnDJ-_z9CTGx"
      },
      "source": [
        "model.evaluate(x_test_pad, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}